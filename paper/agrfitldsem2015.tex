%
% File emnlp2015.tex
%
% Contact: daniele.pighin@gmail.com
%%
%% Based on the style files for ACL-2015, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{color}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{todonotes}


%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Profiling word-sense agreement variation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\section{Introduction}

Some of the disagreement on sense annotation is systematic, and is a result of the interplay between the linguistic properties of the examples and the characteristics of the sense inventory. We propose a regression method to predict systematic disagreement, namely the variation of agreement in sense-annotated examples that depends on the linguistic features of the headword and its contexts.
 In this article we tackle the estimation of the likely agreement of a sense-annotated example by means of regression. \textcolor{red}{ We observe so and so}. 
\end{abstract}

\cite{Artstein2008} provide interpretation of Kripperdorff's $\alpha$ coefficient to describe the reliability of an annotation task, and the way that observed agreement ($A_o$) is calculated for each example. We describe a method to predict the $A_o$ of the examples in our datasets. 

\cite{krippendorff2011} defines disagreement as \textit{by chance}---caused by unavoidable inconsistencies in annotator behavior---and \textit{systematic}---caused by properties of the data being annotated. We know some of the difficult examples in our dataset to have lower agreement, which can be a result of the linguistic characteristics of these examples.

Even though, strictly speaking, the value of $\alpha$ only provides an indication of the replicability of an annotation task, we suggest that the difficulty of annotating a particular example will influence its local observed agreement. Thus, easy examples will have a high $A_o$, that will drop as difficulty increases. 

Identifying low-agreement examples by their linguistic features would help characterize contexts that make words difficult to annotate.


The goal of this experiment is to measure how much of the disagreement in the annotations is caused by linguistic properties of the examples and is thus systematic. More specifically, we intend to interpret the $R^2$ determination coefficient of a regression model on the linguistic features.1

We will consider the proportion of explained variance of the regression model described by the coefficient of determination $R^2$ to be the amount of disagreement our system can give account for, and is thus systematic. 

Detecting the potential agreement examples has a immediate application for data collection, as a way of estimating the proportion of examples of each difficulty level that one wants to sample. 

Moreover, a model of (dis)agreement can help interpret the mispredictions of a word-sense disambiguation system (a la XXX etc al) without requiring the data to be multiply annotated. 

$R^2$ does not strictly say how much agreement is systematic, but how much of the agreement variation within a dataset can be explained by the features. Regression models for datasets that have very low or very high agreement and little variation cannot be so easily fit, because regression fitting benefits from smoothness in the dependent variable, while (discriminative) classification relies on the complementary notion of linear separability. In other words, some datasets lend themselves better to regression than others.



\paragraph{Contributions} \textcolor{red}{This and that}.

\section{Related work}


\cite{Plank2014} 

\cite{Jurgens2014}

\cite{Passonneau2014}

\cite{Jurgens2013}

\cite{Lopez2015} study the agreement-to-performance correlation for WSD. \\

Disagreement between annotators is commonly seen as some kind of featureless noise. However, there are authors that take the stance that, when an item is annotated with low-agreement and the labels it gets assigned are disparate, there is a case for seeing this particular example as difficult to annotate. One of the causes for this difficulty can be regular polysemy.

\cite{Tomuro2001a} provides an interpretation on disagreement between sense-annotated corpora. Her hypothesis is that, when humans disagree on senses of a word, there is an underlying relation between the senses, that is, most inter-annotator disagreement is explained by systematic polysemy. 

Tomuro compares the sense annotations of two corpora, SemCor and DSO, and reports an average K between the sense annotations of the two corpora in their matching sentences of 0.264. She concedes however that a good proportion of the difference is a result of SemCor being annotated by lexicographers and DSO by novices, but claims that these differences provide insight on sense distinctions that are easily misjudged. She is also careful to note that the inverse of her hypothesis does not work, and that systematic polysemy does not cause disagreement per se.  \cite{Jezek2010} also remark that the nouns that cause most disagreement in the coercion annotation task are precisely dot objects.

Note that the $\alpha$ coefficient is an aggregate measure that is obtained dataset-wise, and $A_o$ is the only agreement measure available for each individual example.

\section{Data}
\subsection{Datasets}
We use the following datasets. We restrict our study to the subset of examples of each datasets with at least than two annotations per item. In the datasets with two annotators and one adjudicator, we disregard adjudicator because they potentially have a different bias to the annotators'. All datasets are in English unless otherwise noted.
\noindent\textsc{mascc} The crowdsourced word-sense corpus from \cite{Passenau2010}.\\
\textsc{masce*} The expert annotations for a series of lexical-sample words from \cite{Passonneau2012}, who conducted several annotation rounds. We include the second, third and fourth round of annotation in our experiments. We evaluate on the whole dataset (\textsc{mascew}) as well as on each round's sub-dataset independently, namely \textsc{masc2}, \textsc{masce3} and \textsc{masce4}.\\
\textsc{fntw} The Twitter FrameNet data of \cite{Sogaard2015}. We use the frame-name layer as a word-sense layer, taking the frame-evoking word as target word, and disregard the arguments.\\
\textsc{ensst} The Twitter supersense-annotated data of \newcite{Johannsen2014}.\\
\textsc{eusc} The Basque SemCor \cite{Agirre2006}.\\
\textsc{dasst} The Danish supersense data of \cite{MartinezAlonso2015}.\\
%\end{enumerate}


\begin{table*}[Ht!]

\begin{center}
  \begin{tabular}{lccccccccc}
  \toprule 

Dataset& lang & inventory & task & sentences & instances & annotators & type & $\alpha$ & $A_o$ \\ 
\midrule 

\textsc{mascc} & English & synset & sample & & & & c & \\
\textsc{masew} & English & synset & sample & & & & c & \\
---\textsc{mase2} & English & synset & sample & & & & c & \\
---\textsc{mase3} & English & synset & sample & & & & c & \\
---\textsc{mase4} & English & synset & sample & & & & c & \\
\textsc{ensst} & English & supersense & all-words & & & & c & \\
\textsc{fntw} & English & FrameNet & all-words & & XXX & 3 & e & xxxx & 0.7 \\
\textsc{eusc} & Basque & synset & sample & & & & c & \\
\textsc{dasst} & Danish & supersense & all-words & & & & c & \\


\bottomrule

  \end{tabular}  
\end{center}
\caption{Datasets \label{tab:data} \textcolor{red}{This table needs filling}}
\end{table*} 

\subsection{Features}
We define an \textit{instance} as a sentence with a target word for annotation. If a sentence has $n$ annotated target words, it yields $n$ instances. For each instance, we calculate the following features for a word $w$ and its syntactic parent $p$ in a sentence $s$, organized in feature groups.\\ 
\noindent\textsc{freq} Frequency features (2). We calculate the frequency of $w$ and $p$. We scale the frequencies by calculating $log(rank(x)+1)^{-1}$.\\
\textsc{morph} Morphology features (5). We consider the part-of-speech tag (POS) of $w$, of $p$,and the POS-bigram at the left and at the right of $w$. In order to incorporate information on inflectional complexity, we calculate which proportion of the stem of $w$ is covered by $w$, e.g. the occurrences of `jumping' constitute 22\% of the occurrences of the stem "jump". \\
\textsc{syn} Syntactic features (5). We calculate the number of dependents of $w$ and $p$, and a bag of words for the labels of the dependents of $w$ and $p$. We also include the distance from $w$ to the root node, and the linear distance between $w$ and $p$.\\
\textsc{cont} Context features (5). We calculate the amount length of $s$ in tokens, the proportion of $w$ made up of content words, and a bag of words of the context of $w$, i.e. all the words of $s$ except $w$.In order to capture the specificity of the context, we calculate the sentence-wise idf of each stem in $s$, and provide the maximum idf, and the sum of all idfs in $s$. \\ \\
\textsc{inv} Sense inventory features (2). We calculate the amount of possible senses for $w$, including an additional sense when $w$ could be discarded from the annotation---like the tag `O' for supersenses--- or the right synset was not present in WordNet. We also calculate the sense entropy for each word following \newcite{Yarowsky2002}. 

%\item number of possible senses %\todo[inline]{Maybe a better entropy-based measure?}
%\item length of the sentence; length of the sentence in content words.
%\item generality of $w$ (approximated by the entropy of the softmaxed embedding of $w$, cf Lazaridou.
%\item sense relatedness (wordnet distance between senses, something like this) -- avg, min and max
%\item sense specificity (depth in wordnet or framenet) --- avg, min and max
%\item A bag of words of the context of $w$ %--- \todo[inline]{I really would prefer not use lexical features at all, it makes the system more robust across languages.}
%\end{enumerate}

We use TreeTagger for part-of-speech tagging and TurboParser for dependency parsing. All tagging and parsing models are trained on the Universal Dependencies v1.1, which allows cross-language comparison of features.

\subsection{Target variable}
\paragraph*{Regression} Instance-wise observed agreement ($A_o$) is the target variable for the regression experiments. We calculate  the $A_o$ for each example by calculating the pairwise matches in the annotation and dividing over the amount of pairwise combinations of annotation.
\paragraph*{Classification} Agreement level is the target variable for the classification experiments, where we discretize $A_o$ into the three classes LOW , MID and HIGH. We set the threshold for LOW at $Ao \le \frac{1}{3}$ and for HIGH at $Ao \ge \frac{2}{3}$, while the intermediate values receive the label MID.


\section{Experimental setup}
For regression,we use a L2 regularized regression model
% , which works, but unfortunately the R^2 scores are much lower than what we get from plain linear regression. 
The output of the regression is sigmoid-transformed to keep the values in the range $[0,1]$.
For classification, we train and test a maximum entropy classifier. We use the sklearn\footnote{\url{A}} implemetation for both learning algorithms. We train and test  on 10-fold cross validation.


\section{Results}
\subsection{Regression}



\begin{table}[Ht!]

\begin{center}
  \begin{tabular}{lc|cc}
 \toprule
system & linreg & mean & median \\
 \midrule
 \textsc{mascc} & 0.19 & 0.21 & 0.21 \\
 \textsc{mascew} & 0.31 & 0.32 & 0.37 \\
---\textsc{masce2} & \textbf{0.27} & 0.26 & 0.25 \\
---\textsc{masce3} &\textbf{ 0.36} & 0.29 & 0.20 \\
---\textsc{masce4} & \textbf{0.46} & 0.40 & 0.27 \\
\textsc{ensst} & \textbf{0.43} & 0.35 & 0.31 \\
\textsc{fntw} & \textbf{0.27} & 0.26 & 0.18 \\

\textsc{eusc} & 0.35 & 0.37 & 0.24 \\
\textsc{dasst} & 0.42 & 0.44 & 0.33 \\

\bottomrule

  \end{tabular}  
\end{center}
\caption{Agreement prediction as regression compared against mean and median $A_o$. Datasets where the system outperforms the hardest baseline are marked in bold. \label{tab:regagr_results}}
\end{table} 

For positive values of $R^2$, we can claim that there is at least that much proportion of the disagreement that can be explained by the features and is thus systematic. A smoother target variable is easier to fit. Fewer annotators per item yield more jagger distributions of $A_o$, with fewer different intermediate values. The higher the amount of annotators, the better the resolution of the target variable. 

\subsection{Classification}
\begin{table}[Ht!]

\begin{center}
  \begin{tabular}{lc|ccc}
 \toprule
system & maxent & MFS & stratified & uniform\\ 
 \midrule
 \textsc{mascc} & \textbf{0.45} & 0.27 & 0.35 & 0.37\\ 

 \textsc{mascew} & \textbf{0.48} & 0.39 & 0.39 & 0.35\\ 
---\textsc{masce2} & \textbf{0.39} & 0.25 & 0.34 & 0.33\\ 
---\textsc{masce3} & \textbf{0.62} & 0.60 & 0.57 & 0.53\\ 
---\textsc{masce4} & \textbf{0.63} & 0.62 & 0.62 & 0.55\\ 
\textsc{ensst} & 0.50 & 0.39 & 0.51 & 0.49\\ 
\textsc{fntw} & \textbf{0.71} & 0.63 & 0.61 & 0.51\\ 

\textsc{eusc}  & \textbf{0.68} & 0.65 & 0.63 & 0.54 \\
\textsc{dasst} & \textbf{0.60} & 0.53 & 0.55 & 0.53\\ 

\bottomrule

  \end{tabular}  
\end{center}
\caption{Agreement prediction as classification compared against the most-frequent, stratified and uniform baseline. Datasets where the system outperforms the hardest baseline are marked in bold. \label{tab:classresults}.}
\end{table} 

The two Twitter datasets (XXXX and XXXX) show very different behavior. The framenet dataset has a very large  amount of instances where the amount of possible senses is very low, while the amount of possible senses is always large (XXX) for the Supersense data. \textcolor{red}{The preannotation of the framenet data makes it more streamlined and provides artificial agreement, while the open-guideline free method for the SST data is more exposed to the domain problems of Twitter. I think a lot of the bad performance on TWitter has to do with the badness of the automatically generated features, WHILE for Framenet the sense-inventory features might be the most relevant}


%\begin{figure*}[htt]
%\includegraphics[scale=0.25]{scatterdraft.pdf}\includegraphics[scale=0.25]{scatterdraft.pdf}
%\includegraphics[scale=0.25]{scatterdraft.pdf}
%
%\caption{ \label{fig:reglit_dklocorg}\textcolor{red}{Comparative scatterplots for some datasets}}    
%\end{figure*}

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2015}

\section{Analysis}

\begin{figure}[htt]
\includegraphics[scale=0.3]{corrtable.jpg}

\caption{ \label{fig:correlations}Correlation between the numeric-valued features and $A_o$ for the four English datasets}    
\end{figure}

\section{Conclusions and further work}

We have described a method to model agreement as a continuous value  the observed agreement $A_o$, and as a discrete value. We have conduct experiments on XXXX datasets, which comprise three languages, all-words vs. lexical-sample word annotations, and crowdsourced vs. expert annotations. Our findings reveal that \todo{They do reveal something right?}

 The learnability of the task is limited by the resolution of the target variable, the annotator bias (more relevant for crowdsourcing, as a result of the risk-avoiding strategy many turkers use), and the performance of the predictions for part of speech and syntactic dependencies.


\textcolor{red}{We observe that syntactic features correlate with high agreement, which implies that marked syntactic contexts help an univocal interpretation. Most negative features for agreement are lexical, which indicates that difficult or unusual words get on the way of annotator agreement. However, we have found linguistic cues that pinpoint to syntactic behavior of the headword that cause agreement to drop, because annotators interpret them in different manners. }

This system can be used as an automatic review method for sense-annotation tasks, because it identifies a proportion of systematic disagreement that can be attributed to certain linguistic features, which can lead to reviews of annotation guidelines. 


\section{Further work}
The overall conclusiveness of the study requires expanding this research to more datasets and languages, and well as further exploring the difference in annotator bias between  expert or crowdsourced annotations. Moreover, our feature repertoire does not include characteristics of the sense inventory in terms of sense relatedness like autohyponymy, depth in the sense ontology, or qualitative properties of the potential senses like whether they are abstract. Also, at this current stage, we do not include feature interactions in our models

\bibliographystyle{acl}
\bibliography{biblio}

\end{document}
